Evals is a place where people test and improve OpenAI robots. You can use Evals to check how good a robot is at finishing a task and compare robots to see which one is the best at a task. To use Evals, you need to set up a special key and follow some instructions. There's a special offer where you can use a new robot called GPT-4 if you make a really good test!

To make a test, you need to follow some instructions in order. First, set up your key and computer. Second, learn how to use tests other people made. Third, look at some examples to see how tests are made. Fourth, make your own test by following the instructions. Fifth, see an example of how to do something special with your test.

You can ask questions if you get stuck. Be careful when you use Evals because it can be expensive!

Give a url for each of these python eval sdk libraries, and return a one sentence or less outline per item along with emoji icons and url for each: dependencies = [
    "mypy",
    "openai >= 0.27.2",
    "tiktoken",
    "blobfile",
    "backoff",
    "numpy",
    "snowflake-connector-python[pandas]",
    "pandas",
    "fire",
    "pydantic",
    "tqdm",
    "nltk",
    "filelock",
    "mock",
    "langdetect",
    'termcolor',
    "lz4",
    "pyzstd",
    "pyyaml",
    "sacrebleu",
    "matplotlib",
]

libraries with a brief description and their URLs:

mypy 🎓 (type checking) - http://mypy-lang.org/ - a tool that helps you write better Python code by checking your types and finding errors before you run your code.
openai 🤖 (OpenAI models) - https://pypi.org/project/openai/ - an open-source library that lets you access OpenAI's powerful AI models and algorithms, like GPT-3 and DALL-E.
tiktoken 🔑 (authentication) - https://pypi.org/project/tiktoken/ - a library for generating and managing secure tokens for authentication.
blobfile 🗄️ (file storage) - https://github.com/guardian/blobfile - a library for storing and retrieving files using cloud-based storage services.
backoff 🔄 (error handling) - https://github.com/litl/backoff - a library for adding retries and exponential backoff to your code to handle errors more gracefully.
numpy 🔢 (numerical computing) - https://numpy.org/ - a library for numerical computing in Python, with support for arrays, matrices, and mathematical operations.
snowflake-connector-python[pandas] ☃️ (database connector) - https://pypi.org/project/snowflake-connector-python/ - a library for connecting to Snowflake, a cloud-based data warehouse, and handling data in pandas DataFrames.
pandas 🐼 (data manipulation) - https://pandas.pydata.org/ - a library for data manipulation and analysis, with support for data structures like DataFrames and Series.
fire 🔥 (command-line interface) - https://github.com/google/python-fire - a library for automatically generating command-line interfaces (CLIs) from Python code.
pydantic 🤖 (data validation) - https://pydantic-docs.helpmanual.io/ - a library for data validation and modeling, with support for complex data structures and type annotations.
tqdm 🔄 (progress bar) - https://tqdm.github.io/ - a library for adding progress bars to your code to monitor long-running processes.
nltk 📖 (natural language processing) - https://www.nltk.org/ - a library for natural language processing, with support for tasks like tokenization, part-of-speech tagging, and sentiment analysis.
filelock 🔒 (file locking) - https://github.com/benediktschmitt/py-filelock - a library for creating and managing file locks to prevent conflicts between concurrent processes accessing the same file.
mock 🃏 (testing) - https://github.com/testing-cabal/mock - a library for creating mock objects and functions for unit testing.
langdetect 🌐 (language detection) - https://pypi.org/project/langdetect/ - a library for detecting the language of text.
termcolor 🌈 (text coloring) - https://pypi.org/project/termcolor/ - a library for adding colors and formatting to text in the terminal.
lz4 📦 (compression) - https://github.com/python-lz4/python-lz4 - a library for compressing and decompressing data using the LZ4 algorithm.
pyzstd 📦 (compression) - https://github.com/sergey-dryabzhinsky/python-zstd - a library for compressing and decompressing data using the Zstandard algorithm.
pyyaml 🧪 (data serialization) - https://pyyaml.org


🔹 Use 🐼, 🌈, or 🏃 to make fun buttons and things you can click and move around on the screen!

🔹 Get real-time feedback when you make changes to your code using a 🐕, a 🔁, or a 🔄!

🔹 You can use different languages, like 🤝, 🦄, or 🐍, to talk to your computer and make it do cool things!

🔹 You can use 🐼, 🔢, or 🚀 to move and manipulate numbers and data!

🔹 Keep your data safe by using 🥒, 🛡️, or 🗄️ to save it between different times you use your computer!

🔹 Make pretty graphs and pictures using 📈, 📊, or 🌟!

🔹 Create your own website or app with 🚀, 🍶, or 🎸 and show it to your friends and family!

🔹 Share your work with others and work together using 🐙, 🌐, or 📓!

🔹 Use 🤖, 🔥, or 🧠 to teach your computer to recognize things like pictures and sounds!


🔹 UI components:
Use ipywidgets 🐼, Bokeh 🌈 or Dash 🏃 to create interactive UI components and integrate them with Streamlit's reactive components.
- ipywidgets: https://ipywidgets.readthedocs.io/en/stable/
- Bokeh: https://docs.bokeh.org/en/latest/
- Dash: https://dash.plotly.com/introduction

🔹 Real-time feedback:
Monitor changes to code using watchdog 🐕, autoreload 🔁 or Livereload 🔄, and automatically refresh your Streamlit application for real-time feedback.
- watchdog: https://pythonhosted.org/watchdog/
- autoreload: https://docs.python.org/3/library/autoreload.html
- Livereload: https://github.com/lepture/python-livereload

🔹 Multi-language support:
Integrate R code using reticulate 🤝, Julia code using PyCall 🦄, or C++ code using pybind11 🐍, with your Streamlit application.
- reticulate: https://rstudio.github.io/reticulate/
- PyCall: https://github.com/JuliaPy/PyCall.jl
- pybind11: https://pybind11.readthedocs.io/en/stable/

🔹 Data handling:
Manipulate data in Python using pandas 🐼, numpy 🔢 or dask 🚀, and load it into your Streamlit application.
- pandas: https://pandas.pydata.org/docs/
- numpy: https://numpy.org/doc/
- dask: https://docs.dask.org/en/latest/

🔹 Data persistence:
Persist data between runs of your Streamlit application using pickle 🥒, joblib 🛡️ or h5py 🗄️.
- pickle: https://docs.python.org/3/library/pickle.html
- joblib: https://joblib.readthedocs.io/en/latest/
- h5py: https://docs.h5py.org/en/stable/

🔹 Interactive visualizations:
Create visualizations using plotly 📈, matplotlib 📊 or altair 🌟, and display them in your Streamlit application.
- plotly: https://plotly.com/python/
- matplotlib: https://matplotlib.org/
- altair: https://altair-viz.github.io/

🔹 API generation:
Build APIs using FastAPI 🚀, Flask 🍶 or Django 🎸, and use Streamlit to display and interact with your API.
- FastAPI: https://fastapi.tiangolo.com/
- Flask: https://flask.palletsprojects.com/en/2.1.x/
- Django: https://www.djangoproject.com/

🔹 Collaboration and sharing:
Share and collaborate on Streamlit projects using GitHub 🐙, Google Colab 🌐 or Jupyter Notebook 📓.
- GitHub: https://docs.github.com/en
- Google Colab: https://colab.research.google.com/notebooks/intro.ipynb
- Jupyter Notebook: https://jupyter.org/documentation

🔹 Integration with popular libraries:
Leverage TensorFlow 🤖, PyTorch 🔥 or Scikit-learn 🧠 in your Streamlit application.
- TensorFlow: https://www.tensorflow.org/api_docs
- PyTorch: https://pytorch.org/docs/stable/index.html
- Scikit-learn: https://scikit-learn.org/stable/



🤖 Evals is a place where people test and improve OpenAI robots!

🔎 You can use Evals to check how good a robot is at finishing a task.

🏆 You can compare robots to see which one is the best at a task.

💻 To use Evals, you need to set up a special key and follow some instructions.

🔥 There's a special offer where you can use a new robot called GPT-4 if you make a really good test!

👉 You can learn how to make tests by following the instructions in this order:
    1. Set up your key and computer.
    2. Learn how to use tests other people made.
    3. Look at some examples to see how tests are made.
    4. Make your own test by following the instructions.
    5. See an example of how to do something special with your test.

📝 You can ask questions if you get stuck.

🚨 Flashy Light activate!


# Evals

Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.

You can use Evals to create and run evaluations that:
- use datasets to generate prompts,
- measure the quality of completions provided by an OpenAI model, and
- compare performance across different datasets and models.

With Evals, we aim to make it as simple as possible to build an eval while writing as little code as possible. To get started, we recommend that you follow these steps **in order**:
1. Read through this doc and follow the [setup instructions below](README.md#Setup).
2. Learn how to run existing evals: [run-evals.md](docs/run-evals.md).
3. Familiarize yourself with the existing eval templates: [eval-templates.md](docs/eval-templates.md).
4. Walk through the process for building an eval: [build-eval.md](docs/build-eval.md)
5. See an example of implementing custom eval logic: [custom-eval.md](docs/custom-eval.md).

If you think you have an interesting eval, please open a PR with your contribution. OpenAI staff actively review these evals when considering improvements to upcoming models.

____________________
🚨 For a limited time, we will be granting GPT-4 access to those who contribute high quality evals. Please follow the instructions mentioned above and note that spam or low quality submissions will be ignored❗️

Access will be granted to the email address associated with an accepted Eval. Due to high volume, we are unable to grant access to any email other than the one used for the pull request.
____________________

## Setup

To run evals, you will need to set up and specify your OpenAI API key. You can generate one at <https://platform.openai.com/account/api-keys>. After you obtain an API key, specify it using the `OPENAI_API_KEY` environment variable. **Please be aware of the [costs](https://openai.com/pricing) associated with using the API when running evals.**

**Minimal Required Version: Python 3.9**

### Downloading evals

Our Evals registry is stored using [Git-LFS](https://git-lfs.com/). Once you have downloaded and installed LFS, you can fetch the evals with:
```sh
git lfs fetch --all
git lfs pull
```

You may just want to fetch data for a select eval. You can achieve this via:
```sh
git lfs fetch --include=evals/registry/data/${your eval}
git lfs pull
```

### Making evals

If you are going to be creating evals, we suggest cloning this repo directly from GitHub and installing the requirements using the following command:

```sh
pip install -e .
```

Using `-e`, changes you make to your eval will be reflected immediately without having to reinstall.

### Running evals

If you don't want to contribute new evals, but simply want to run them locally, you can install the evals package via pip:

```sh
pip install evals
```

We provide the option for you to log your eval results to a Snowflake database, if you have one or wish to set one up. For this option, you will further have to specify the `SNOWFLAKE_ACCOUNT`, `SNOWFLAKE_DATABASE`, `SNOWFLAKE_USERNAME`, and `SNOWFLAKE_PASSWORD` environment variables.

## FAQ

Do you have any examples of how to build an eval from start to finish?

- Yes! These are in the `examples` folder. We recommend that you also read through [build-eval.md](docs/build-eval.md) in order to gain a deeper understanding of what is happening in these examples.

Do you have any examples of evals implemented in multiple different ways?

- Yes! In particular, see `evals/registry/evals/coqa.yaml`. We have implemented small subsets of the [CoQA](https://stanfordnlp.github.io/coqa/) dataset for various eval templates to help illustrate the differences.

When I run an eval, it sometimes hangs at the very end (after the final report). What's going on?

- This is a known issue, but you should be able to interrupt it safely and the eval should finish immediately after.

There's a lot of code, and I just want to spin up a quick eval. Help? OR,

I am a world-class prompt engineer. I choose not to code. How can I contribute my wisdom?

- If you follow an existing [eval template](docs/eval-templates.md) to build a basic or model-graded eval, you don't need to write any evaluation code at all! Just provide your data in JSON format and specify your eval parameters in YAML. [build-eval.md](docs/build-eval.md) walks you through these steps, and you can supplement these instructions with the Jupyter notebooks in the `examples` folder to help you get started quickly. Keep in mind, though, that a good eval will inevitably require careful thought and rigorous experimentation!

## Disclaimer

By contributing to Evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an Eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI Evals will be subject to our usual Usage Policies: https://platform.openai.com/docs/usage-policies.
